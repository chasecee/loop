#!/usr/bin/env python3
"""
Upload Coordinator Performance Test - Pi Edition

Tests upload coordinator performance on Raspberry Pi hardware.
Validates the ZIP processing optimizations work as expected.

Usage:
    cd /home/pi/loop/backend
    python test_upload_performance.py
"""

import asyncio
import json
import time
import tempfile
import zipfile
import sys
import os
from pathlib import Path
from typing import Dict, Any
from datetime import datetime

# Add backend to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Also add the parent directory to ensure web module is found
backend_dir = os.path.dirname(os.path.abspath(__file__))
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

# Ensure web package is importable
web_dir = os.path.join(backend_dir, 'web')
if os.path.exists(web_dir) and web_dir not in sys.path:
    sys.path.insert(0, web_dir)

print(f"üîç Python path includes: {backend_dir}")
print(f"üîç Web directory exists: {os.path.exists(web_dir)}")

# Mock the required modules for testing
class MockBroadcaster:
    async def upload_progress_simple(self, filename: str, progress: int, status: str):
        print(f"üìä Progress: {filename} - {progress}% - {status}")
    
    async def media_uploaded(self, metadata: Dict[str, Any]):
        print(f"üì§ Media uploaded: {metadata.get('slug', 'unknown')}")
    
    async def loop_updated(self, loop_data: list):
        print(f"üîÑ Loop updated: {len(loop_data)} items")

class MockMediaIndex:
    def __init__(self):
        self.media = {}
    
    def get_media_dict(self):
        return self.media
    
    def add_media(self, metadata: Dict[str, Any], make_active: bool = True):
        slug = metadata.get('slug')
        if slug:
            self.media[slug] = metadata
        print(f"üìö Added to media index: {slug} (active={make_active})")
    
    def remove_media(self, slug: str):
        if slug in self.media:
            del self.media[slug]
        print(f"üóëÔ∏è Removed from media index: {slug}")

async def create_test_zip(frame_count: int = 100) -> bytes:
    """Create a test ZIP file with frames for Pi performance testing."""
    print(f"üèóÔ∏è Creating test ZIP with {frame_count} frames...")
    start_time = time.time()
    
    with tempfile.TemporaryDirectory() as tmpdir:
        zip_path = Path(tmpdir) / "test_frames.zip"
        
        with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
            # Add metadata
            metadata = {
                "original_filename": "test_video.mp4",
                "frame_count": frame_count,
                "width": 320,
                "height": 240,
                "type": "video/mp4"
            }
            zf.writestr("metadata.json", json.dumps(metadata))
            
            # Add test frames (realistic RGB565 frame data for Pi)
            frame_data = b'\x00\xFF' * (320 * 240)  # RGB565 frame data (153.6KB per frame)
            for i in range(frame_count):
                zf.writestr(f"frame_{i:04d}.rgb", frame_data)
        
        # Read ZIP content
        with open(zip_path, 'rb') as f:
            zip_content = f.read()
    
    creation_time = time.time() - start_time
    print(f"‚úÖ Created test ZIP: {len(zip_content)/1024/1024:.1f}MB in {creation_time:.2f}s")
    return zip_content

async def test_upload_performance():
    """Test upload coordinator performance on Pi hardware."""
    
    print("üß™ Upload Coordinator Performance Test - Pi Edition")
    print("=" * 60)
    print(f"üîß Running on: {os.uname().machine} ({os.uname().sysname})")
    print(f"üìÅ Working directory: {os.getcwd()}")
    
    # Mock the dependencies
    from unittest.mock import patch
    
    try:
        # Try multiple import strategies for Pi compatibility
        upload_coordinator_module = None
        coordinator = None
        
        # Strategy 1: Direct import
        try:
            from web.core.upload_coordinator import UploadCoordinator, UploadTransaction
            upload_coordinator_module = "web.core.upload_coordinator"
            print("‚úÖ Strategy 1: Direct import successful")
            
            # Mock and create coordinator
            with patch(f'{upload_coordinator_module}.broadcaster', MockBroadcaster()), \
                 patch(f'{upload_coordinator_module}.media_index', MockMediaIndex()):
                coordinator = UploadCoordinator()
                await run_performance_tests(coordinator, UploadTransaction)
                
        except ImportError as e1:
            print(f"‚ùå Strategy 1 failed: {e1}")
            
            # Strategy 2: Add web to path and import
            try:
                web_path = os.path.join(os.path.dirname(__file__), 'web')
                if web_path not in sys.path:
                    sys.path.insert(0, web_path)
                from core.upload_coordinator import UploadCoordinator, UploadTransaction
                upload_coordinator_module = "core.upload_coordinator"
                print("‚úÖ Strategy 2: Web path import successful")
                
                # Mock and create coordinator
                with patch(f'{upload_coordinator_module}.broadcaster', MockBroadcaster()), \
                     patch(f'{upload_coordinator_module}.media_index', MockMediaIndex()):
                    coordinator = UploadCoordinator()
                    await run_performance_tests(coordinator, UploadTransaction)
                    
            except ImportError as e2:
                print(f"‚ùå Strategy 2 failed: {e2}")
                
                # Strategy 3: Direct file import
                try:
                    import importlib.util
                    upload_coord_path = os.path.join(os.path.dirname(__file__), 'web', 'core', 'upload_coordinator.py')
                    spec = importlib.util.spec_from_file_location("upload_coordinator", upload_coord_path)
                    upload_coord = importlib.util.module_from_spec(spec)
                    sys.modules["upload_coordinator"] = upload_coord
                    spec.loader.exec_module(upload_coord)
                    UploadCoordinator = upload_coord.UploadCoordinator
                    UploadTransaction = upload_coord.UploadTransaction
                    print("‚úÖ Strategy 3: Direct file import successful")
                    
                    # Mock the globals directly
                    upload_coord.broadcaster = MockBroadcaster()
                    upload_coord.media_index = MockMediaIndex()
                    coordinator = UploadCoordinator()
                    await run_performance_tests(coordinator, UploadTransaction)
                    
                except Exception as e3:
                    print(f"‚ùå Strategy 3 failed: {e3}")
                    raise ImportError(f"All import strategies failed: {e1}, {e2}, {e3}")

    except ImportError as e:
        print(f"‚ùå Failed to import upload coordinator: {e}")
        print("   Make sure you're running from the backend directory")
        print(f"   Current directory: {os.getcwd()}")
        print("   Available files:")
        try:
            for f in os.listdir('.'):
                if f in ['web', 'utils', 'main.py']:
                    print(f"     ‚úÖ {f}")
        except:
            pass
    except Exception as e:
        print(f"‚ùå Test failed with unexpected error: {e}")

async def run_performance_tests(coordinator, UploadTransaction):
    """Run the actual performance tests with the provided coordinator."""
    
    print("‚úÖ Upload coordinator ready - starting tests...")
    
    # Pi-optimized test cases (smaller sizes for Pi Zero 2)
    test_cases = [
        {"name": "Small ZIP (5 frames)", "frame_count": 5, "expected_time": 1.0},
        {"name": "Medium ZIP (25 frames)", "frame_count": 25, "expected_time": 3.0},
        {"name": "Large ZIP (100 frames)", "frame_count": 100, "expected_time": 10.0},
    ]
    
    print(f"\nüéØ Running {len(test_cases)} performance tests...")
    
    overall_start = time.time()
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"üî¨ Test {i}/{len(test_cases)}: {test_case['name']}")
        print(f"üéØ Target: <{test_case['expected_time']:.1f}s")
        print("-" * 60)
        
        # Create test data
        zip_content = await create_test_zip(test_case['frame_count'])
        
        # Mock file info
        file_info = {
            'filename': f"test_{test_case['frame_count']}_frames.zip",
            'content': zip_content,
            'hash': f"test_hash_{test_case['frame_count']}",
            'size': len(zip_content),
            'content_type': 'application/zip'
        }
        
        # Create mock transaction
        transaction = UploadTransaction(
            id=f"test_{test_case['frame_count']}",
            files=[{'filename': file_info['filename'], 'hash': file_info['hash'], 'size': file_info['size']}],
            state='processing',
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        # Test the optimized ZIP processing
        print(f"üèÉ Starting ZIP processing...")
        start_time = time.time()
        
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                processed_dir = Path(tmpdir)
                
                result_slug = await coordinator._process_zip_file(
                    transaction, file_info, processed_dir
                )
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                # Verify results
                final_dir = processed_dir / result_slug
                frames_dir = final_dir / "frames"
                
                frame_count = len(list(frames_dir.glob("*.rgb"))) if frames_dir.exists() else 0
                
                # Calculate metrics
                frames_per_sec = test_case['frame_count'] / processing_time
                throughput_mbps = len(zip_content) / 1024 / 1024 / processing_time
                vs_target = processing_time / test_case['expected_time']
                
                # Performance rating
                if processing_time <= test_case['expected_time'] * 0.5:
                    rating = "üöÄ EXCELLENT"
                elif processing_time <= test_case['expected_time']:
                    rating = "‚úÖ GOOD"
                elif processing_time <= test_case['expected_time'] * 2:
                    rating = "‚ö†Ô∏è ACCEPTABLE"
                else:
                    rating = "üêå SLOW"
                
                print(f"‚úÖ Processing completed successfully!")
                print(f"   ‚è±Ô∏è  Time: {processing_time:.2f}s (target: {test_case['expected_time']:.1f}s)")
                print(f"   üèÅ Speed: {frames_per_sec:.1f} frames/sec")
                print(f"   üìà Throughput: {throughput_mbps:.1f} MB/s")
                print(f"   üìÅ Extracted: {frame_count}/{test_case['frame_count']} frames")
                print(f"   üéØ vs Target: {vs_target:.1f}x")
                print(f"   üèÜ Rating: {rating}")
                
                # Store results
                results.append({
                    'name': test_case['name'],
                    'time': processing_time,
                    'target': test_case['expected_time'],
                    'frames': frame_count,
                    'throughput': throughput_mbps,
                    'rating': rating,
                    'success': True
                })
                
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            print(f"‚ùå Processing failed after {processing_time:.2f}s")
            print(f"   Error: {str(e)}")
            
            results.append({
                'name': test_case['name'],
                'time': processing_time,
                'target': test_case['expected_time'],
                'error': str(e),
                'success': False
            })
    
    # Summary report
    overall_time = time.time() - overall_start
    print(f"\n{'='*60}")
    print(f"üèÅ PERFORMANCE TEST SUMMARY")
    print(f"{'='*60}")
    print(f"‚è±Ô∏è  Total test time: {overall_time:.2f}s")
    print(f"üß™ Tests run: {len(results)}")
    
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    print(f"‚úÖ Successful: {len(successful)}")
    print(f"‚ùå Failed: {len(failed)}")
    
    if successful:
        avg_time = sum(r['time'] for r in successful) / len(successful)
        avg_throughput = sum(r.get('throughput', 0) for r in successful) / len(successful)
        print(f"üìä Average processing time: {avg_time:.2f}s")
        print(f"üìä Average throughput: {avg_throughput:.1f} MB/s")
    
    print(f"\nüìã Detailed Results:")
    for result in results:
        status = "‚úÖ" if result['success'] else "‚ùå"
        print(f"   {status} {result['name']}: {result['time']:.2f}s")
        if result['success']:
            print(f"      {result['rating']}")
    
    # Recommendations
    print(f"\nüí° RECOMMENDATIONS:")
    if all(r['success'] for r in results):
        if all(r['time'] <= r['target'] for r in successful):
            print("   üéâ All tests passed! Upload coordinator is well optimized.")
        else:
            slow_tests = [r for r in successful if r['time'] > r['target']]
            print(f"   ‚ö†Ô∏è  {len(slow_tests)} test(s) exceeded target times.")
            print("   Consider further optimization for larger files.")
    else:
        print("   ‚ùå Some tests failed. Check error messages above.")
        print("   Verify all dependencies are properly installed.")
    
    if any(r.get('throughput', 0) < 1.0 for r in successful):
        print("   üìà Low throughput detected. Check SD card performance:")
        print("      sudo hdparm -t /dev/mmcblk0p1")

if __name__ == "__main__":
    print("üîß Starting upload coordinator performance test on Pi...")
    asyncio.run(test_upload_performance()) 